{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b304f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics as met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f93d13",
   "metadata": {},
   "source": [
    "1. Create a new file named model_evaluation.py or model_evaluation.ipynb for these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59184219",
   "metadata": {},
   "source": [
    "2.Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13244df4",
   "metadata": {},
   "source": [
    "In the context of this problem, what is a false positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f821778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive case = dog\n",
    "#negative case = cat\n",
    "\n",
    "#if dog is our positive case, false positive is when your predict a dog but it was actually a cat\n",
    "#which is 13 on this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255efd1a",
   "metadata": {},
   "source": [
    "In the context of this problem, what is a false negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d7e398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if dog is our positive case, false negative is prediction a cat but it was a dog actually which is 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0db3b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=46\n",
    "tn=34\n",
    "fp=13\n",
    "fn=7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a9520",
   "metadata": {},
   "source": [
    "How would you describe this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8f84e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive case = dog\n",
      "precision 0.78\n",
      "accuracy 0.58\n",
      "recall 0.87\n"
     ]
    }
   ],
   "source": [
    "precision = tp/(tp+fp)\n",
    "accuracy = (tp+tn)/(tp+tn+tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "print(\"positive case = dog\")\n",
    "print('precision', round(precision,2))\n",
    "print('accuracy', round(accuracy,2))\n",
    "print('recall', round(recall,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b8d38",
   "metadata": {},
   "source": [
    "3.You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "\n",
    "Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a71595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#postive case = defective duck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96aa87ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load csv\n",
    "df = pd.read_csv(\"c3.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba370e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not only true postives(defective ducks), we need to track down ducks we think are good but are infact defective,\n",
    "#which is false negative. so we use recall here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "373fb97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "              Defect   No Defect  accuracy   macro avg  weighted avg\n",
      "precision   0.800000    0.957895      0.95    0.878947      0.945263\n",
      "recall      0.500000    0.989130      0.95    0.744565      0.950000\n",
      "f1-score    0.615385    0.973262      0.95    0.794323      0.944632\n",
      "support    16.000000  184.000000      0.95  200.000000    200.000000\n",
      "\n",
      "model2\n",
      "              Defect   No Defect  accuracy   macro avg  weighted avg\n",
      "precision   0.100000    0.936364      0.56    0.518182      0.869455\n",
      "recall      0.562500    0.559783      0.56    0.561141      0.560000\n",
      "f1-score    0.169811    0.700680      0.56    0.435246      0.658211\n",
      "support    16.000000  184.000000      0.56  200.000000    200.000000\n",
      "\n",
      "model3\n",
      "              Defect   No Defect  accuracy   macro avg  weighted avg\n",
      "precision   0.131313    0.970297     0.555    0.550805      0.903178\n",
      "recall      0.812500    0.532609     0.555    0.672554      0.555000\n",
      "f1-score    0.226087    0.687719     0.555    0.456903      0.650789\n",
      "support    16.000000  184.000000     0.555  200.000000    200.000000\n"
     ]
    }
   ],
   "source": [
    "#we can use classification report on all there models\n",
    "print('model1')\n",
    "print(pd.DataFrame(met.classification_report(df.actual, df.model1, output_dict=True)))\n",
    "print()\n",
    "print('model2')\n",
    "print(pd.DataFrame(met.classification_report(df.actual, df.model2, output_dict=True)))\n",
    "print()\n",
    "print('model3')\n",
    "print(pd.DataFrame(met.classification_report(df.actual, df.model3, output_dict=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2022849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since recall for model 3 is the highest,81.25% , we will use model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfdda73",
   "metadata": {},
   "source": [
    "Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de57ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, postive case is defective ducks.\n",
    "#pr team also wants to catch false postive, ie. giving out good ducks when it is not\n",
    "#so we use precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7042aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "as in the table, model1 has highest precision, 80% so we choose it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61455691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
